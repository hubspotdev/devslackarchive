[
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 06:26:42",
    "text": "Hubdb api -- is there a clear all data in the table? I'm going to hit my rate limit easily if I'm removing each row one by one"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 06:31:31",
    "text": "And as far as I'm aware, the DELETE table doesn't really delete it, it removes it from the UI, but the table and all the data is still in there"
  },
  {
    "poster": {
      "poster_name": "Jeff Boulter",
      "poster_handle": "boulter"
    },
    "time_stamp": "2017-04-12 07:45:12",
    "text": "That’s correct, we soft-delete tables"
  },
  {
    "poster": {
      "poster_name": "Jeff Boulter",
      "poster_handle": "boulter"
    },
    "time_stamp": "2017-04-12 07:46:51",
    "text": "No public API yet for truncating a table, but that’s an option when you import a CSV into a table"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 07:56:04",
    "text": "Yeah, I'm trying to figure out a way to update 53 tables once a month... and taking a spreadsheet and saving it into 53 csv files based upon state filters and uploading them individually and mapping the columns isn't going to work"
  },
  {
    "text": "<@U22PJPGK0|boulter> uploaded a file: <https://HubSpotdev.slack.com/files/boulter/F4XJX8FDW/here___s_some_quick_documentation_on_the_import_endpoint.html|here’s some quick documentation on the import endpoint>",
    "time_stamp": "2017-04-12 09:56:57"
  },
  {
    "poster": {
      "poster_name": "Jeff Boulter",
      "poster_handle": "boulter"
    },
    "time_stamp": "2017-04-12 09:57:43",
    "text": "We’ll get that publicly documented too"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:09:11",
    "text": "Right, I'm sayign that is all well and good. But I can't remove each table row by row"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:09:24",
    "text": "Sorry if my brain dump wasn't guided"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:09:45",
    "text": "--and again, I can remove each table row by row, but it won't work like that in teh end"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:09:49",
    "text": "it isnt' ideal"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:10:13",
    "text": "so right now the method is manually uploading the individual csv files"
  },
  {
    "poster": {
      "poster_name": "Jeff Boulter",
      "poster_handle": "boulter"
    },
    "time_stamp": "2017-04-12 10:31:17",
    "text": "wouldn’t the resetTable option in the import options described above do what you want?"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:38:16",
    "text": "I misread what you had there. I see this is new documentation. I thought you said this was well documented. yes, this is exactly what I could use. Hmm, so this uses a csv file. \n\nFrom the master, I could programatiicaly create all of the csv by looping through  the master, creating an object for each state, adding a location field based upon the lat and long, then saving the csv. Then looping again, uploading the csv"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:38:39",
    "text": "Yeah, I think that might work ,then I can just take their current xls file and work with that to magically make this happen."
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 10:38:49",
    "text": "thanks so much"
  },
  {
    "poster": {
      "poster_name": "Jeff Boulter",
      "poster_handle": "boulter"
    },
    "time_stamp": "2017-04-12 11:27:04",
    "text": "it should be relatively smart about importing stuff. I think if you just import a string like “123.34,-34.345”, it’ll parse it into a location object"
  },
  {
    "poster": {
      "poster_name": "Nicholas Decker - Level -22",
      "poster_handle": "nickdeckerdevs"
    },
    "time_stamp": "2017-04-12 19:32:40",
    "text": "they have them separated into two columns"
  }
]